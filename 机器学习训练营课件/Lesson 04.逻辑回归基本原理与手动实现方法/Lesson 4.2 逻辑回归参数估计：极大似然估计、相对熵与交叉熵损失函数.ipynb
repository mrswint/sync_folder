{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.2 逻辑回归参数估计：极大似然估计、相对熵与交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在模型基本结构构建完成之后，接下来我们开始讨论如何进行逻辑回归的参数估计。所谓参数估计，其实就是模型参数求解的更加具有统计学风格的称呼。根据逻辑回归的基本公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1}{1+e^{-(\\hat w^T \\cdot \\hat x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难看出，逻辑回归的参数其实就是线性方程中的自变量系数和截距。不过由于加入了联系函数，逻辑回归的参数并不能像线性回归一样利用最小二乘法进行快速求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，和所有的机器学习模型一样，要求解模型参数，就先必须构造损失函数，然后根据损失函数的基本情况寻找优化算法求解。对于逻辑回归来说，课上将介绍两种不同的方法来创建和求解损失函数，两种方法出发点各不相同但却殊途同归：分别是极大似然估计（Maximum Likelihood Estimate）和通过相对熵（relative entropy）构建交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义模块\n",
    "from ML_basic_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、逻辑回归参数估计基本思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;比较有趣的一点是，尽管逻辑回归的损失函数构建过程比较复杂，但逻辑回归的损失函数的基本形式比较容易理解。因此首先我们先通过一个简单的例子来讨论关于逻辑回归的参数估计的基本思路，即损失函数构建和求解的一般思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.构建损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;现有简单数据集如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|sepal_length|species|      \n",
    "|:--:|:--:|   \n",
    "|1|0|      \n",
    "|3|1|\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于只有一个特征，因此可以构建逻辑回归模型为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y=sigmoid(wx+b)=\\frac{1}{1+e^{-(wx+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将模型输出结果视作概率，则分别带入两条数据可得模型输出结果为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=1|x=1)=\\frac{1}{1+e^{-(w+b)}}$$\n",
    "$$p(y=1|x=3)=\\frac{1}{1+e^{-(3w+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$p(y=1|x=1)$表示$x$取值为1时$y$取值为1的条件概率。而我们知道，两条数据的真实情况为第一条数据$y$取值为0，而第二条数据$y$取值为1，因此我们可以计算$p(y=0|x=1)$如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=0|x=1) = 1-p(y=1|x=1)=1-\\frac{1}{1+e^{-(w+b)}}=\\frac{e^{-(w+b)}}{1+e^{-(w+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|sepal_length|species|1-predict|0-predict|      \n",
    "|:--:|:--:|:--:|:--:|   \n",
    "|1|0|$\\frac{1}{1+e^{-(w+b)}}$|$\\frac{e^{-(w+b)}}{1+e^{-(w+b)}}$|      \n",
    "|3|1|$\\frac{1}{1+e^{-(3w+b)}}$|$\\frac{e^{-(3w+b)}}{1+e^{-(3w+b)}}$|\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一般来说，损失函数的构建目标和模型评估指标保持一致（例如SSELoss和SSE），对于大多数分类模型来说，模型预测的准确率都是最基础的评估指标。此处如果我们希望模型预测结果尽可能准确，就等价于希望$p(y=0|x=1)$和$p(y=1|x=1)$两|个概率结果越大越好。该目标可以统一在求下式最大值的过程中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=0|x=1)\\cdot p(y=1|x=3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即我们希望x取1时y取0和x取3时y取1的同时发生的概率越大越好。      \n",
    "&emsp;&emsp;此外，考虑到损失函数一般都是求最小值，因此可将上式求最大值转化为对应负数结果求最小值，同时累乘也可以转化为对数相加结果，因此上式求最大值可等价于下式求最小值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "LogitLoss(w, b)&=-ln(p(y=1|x=3))-ln(p(y=0|x=1)) \\\\\n",
    "&=-ln(\\frac{1}{1+e^{-(3w+b)}})- ln(\\frac{e^{-(w+b)}}{1+e^{-(w+b)}}) \\\\\n",
    "&=ln(1+e^{-(3w+b)})+ln(1+\\frac{1}{e^{-(w+b)}}) \\\\\n",
    "&=ln(1+e^{-(3w+b)}+e^{(w+b)}+e^{-2w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我们即构建了一个由两条数据所构成的逻辑回归损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 回顾此前课程内容：损失函数和带入数据量息息相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;注意，在上述损失函数的构建过程中有两个关键步骤，需要再次提醒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其一是在将模型高准确率的诉求具象化为$p(y=0|x=1)\\cdot p(y=1|x=3)$参数的过程，此处我们为何不能采用类似SSE的计算思路取构建损失函数，即进行如下运算：$$||y-yhat||_2^2=||y-\\frac{1}{1+e^{-(\\hat w^T \\cdot \\hat x)}}||_2^2$$      \n",
    "&emsp;&emsp;我们一般不会采用该方法构建损失函数，其根本原因在于，在数学层面上我们可以证明，对于逻辑回归，当y属于0-1分类变量时，$||y-yhat||_2^2$损失函数并不是凸函数，而非凸的损失函数将对后续参数最优解求解造成很大麻烦。而相比之下，概率连乘所构建的损失函数是凸函数，可以快速求解出全域最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其二，在构建损失函数的过程中，我们需要将概率连乘改为对数累加，有一个很重要的原因是，在实际建模运算过程中，尤其是面对大量数据进行损失函数构建过程中，由于有多少条数据就要进行多少次累乘，而累乘的因子又是介于(0,1)之间的数，因此极有可能累乘得到一个非常小的数，而通用的计算框架计算精度有限，即有可能在累乘的过程中损失大量精度，而转化为对数累加之后能够很好的避免该问题的发生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.损失函数求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从数学角度可以证明，按照上述构成构建的逻辑回归损失函数仍然是凸函数，此时我们仍然可以通过对LogitLoss(w,b)求偏导然后令偏导函数等于0、再联立方程组的方式来对参数进行求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial LogitLoss(w,b)}{\\partial w}=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial LogitLoss(w,b)}{\\partial b}=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;值得一提的是，上述构建损失函数和求解损失函数的过程，也被称为极大似然估计。接下来我们就将极大似然估计的方法推广到一般过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、利用极大似然估计进行参数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们考虑更为一般的情况，围绕逻辑回归方程的一般形式，采用极大似然估计方法进行参数估计："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归模型：$$y = \\frac{1}{1+e^{-(\\hat w^T \\cdot \\hat x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：$$\\hat w = [w_1,w_2,...w_d, b]^T, \\hat x = [x_1,x_2,...x_d, 1]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解过程总共分为四个步骤，分别是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1.确定似然项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓似然函数，可简单理解为前例中累乘的函数。而累乘过程中的每个项，可称为似然项，不难发现，似然项其实和数据是一一对应的，带入多少条数据进行建模，似然函数中就有多少个似然项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们知道，对于逻辑回归来说，当$\\hat w$和$\\hat x$取得一组之后，既可以有一个概率预测输出结果，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=1|\\hat x;\\hat w) = \\frac{1}{1+e^{-(\\hat w^T \\cdot \\hat x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对应$y$取0的概率为：$$1-p(y=1|\\hat x;\\hat w) =1- \\frac{1}{1+e^{-(\\hat w^T \\cdot \\hat x)}}=\\frac{e^{-(\\hat w^T \\cdot \\hat x)}}{1+e^{-(\\hat w^T \\cdot \\hat x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以令$$p_1(\\hat x;\\hat w)=p(y=1|\\hat x;\\hat w)$$\n",
    "$$p_0(\\hat x;\\hat w)=1-p(y=1|\\hat x;\\hat w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，第$i$个数据所对应的似然项可以写成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_1(\\hat x;\\hat w)^{y_i} \\cdot p_0(\\hat x;\\hat w)^{(1-y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中，$y_i$表示第$i$条数据对应的类别标签。不难发现，当$y_i=0$时，代表的是$i$第条数据标签为0，此时需要带入似然函数的似然项是$p_0(\\hat x;\\hat w)$（因为希望$p_0$的概率更大）。反之，当$y_i=1$时，代表的是$i$第条数据标签为1，此时需要带入似然函数的似然项是$p_1(\\hat x;\\hat w)$。上述似然项可以同时满足这两种不同的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2.构建似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，通过似然项的累乘计算极大似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod^N_{i=1}[p_1(\\hat x;\\hat w)^{y_i} \\cdot p_0(\\hat x;\\hat w)^{(1-y_i)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3.进行对数转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后即可在似然函数基础上对其进行（以e为底的）对数转换，为了方便后续利用优化方法求解最小值，同样我们考虑构建负数对数似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "L(\\hat w) &= -ln(\\prod^N_{i=1}[p_1(\\hat x;\\hat w)^{y_i} \\cdot p_0(\\hat x;\\hat w)^{(1-y_i)}]) \\\\\n",
    "&= \\sum^N_{i=1}[-y_i \\cdot ln(p_1(\\hat x;\\hat w))-(1-y_i) \\cdot ln(p_0(\\hat x;\\hat w))] \\\\\n",
    "&= \\sum^N_{i=1}[-y_i \\cdot ln(p_1(\\hat x;\\hat w))-(1-y_i) \\cdot ln(1-p_1(\\hat x;\\hat w))] \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式推导致此即可，后续我们将借助该公式进行损失函数求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4.求解对数似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过一系列数学过程可以证明，通过极大似然估计构建的损失函数是凸函数，此时我们可以采用导数为0联立方程组的方式进行求解，这也是极大似然估计对参数求解的一般方法。但这种方法会涉及大量的导数运算、方程组求解等，并不适用于大规模甚至是超大规模数值运算，因此，在机器学习领域，我们通常会采用一些更加通用的优化方法对逻辑回归的损失函数进行求解，通常来说是牛顿法或者梯度下降算法，其中，梯度下降算法是机器学习中最为通用的求解损失函数的优化算法，我们将在下一小节花费一整节的时间来进行介绍。本节我们将继续介绍另外一种推导逻辑回归损失函数的方法——KL离散度计算法，并介绍有关信息熵、交叉熵等关键概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于模型本身和损失函数构建方式都和线性回归有所不同，逻辑回归的损失函数无法采用最小二乘法进行求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、熵、相对熵与交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们介绍另一种构建逻辑回归损失函数的基本思路——借助相对熵（relative entropy，又称KL离散度）构建损失函数。尽管最终损失函数构建结果和极大似然估计相同，但该过程所涉及到的关于信息熵（entropy）、相对熵等概念却是包括EM算法、决策树算法等诸多机器学习算法的理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.熵（entropy）的基本概念与计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通常我们用熵（entropy）来表示随机变量不确定性的度量，或者说系统混乱程度、信息混乱程度。熵的计算公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) = -\\sum^n_{i=1}p(x_i)log(p(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$p(x_i)$表示多分类问题中第$i$个类别出现的概率，$n$表示类别总数，通常来说信息熵的计算都取底数为2，并且规定$log0=0$。举例说明信息熵计算过程，假设有二分类数据集1标签如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>数据集1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|labels|      \n",
    "|:--:|:--:|   \n",
    "|1|0|      \n",
    "|2|1|\n",
    "|3|1|\n",
    "|4|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则信息熵的计算过程中$n=2$，令$p(x_1)$表示类别0的概率，$p(x_2)$表示类别1的概率（反之亦然），则$$p(x_1)=\\frac{1}{4}$$\n",
    "$$p(x_2)=\\frac{3}{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则该数据集的信息熵计算结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "H(X) &= -(p(x_1)log(p(x_1))+p(x_2)log(p(x_2))) \\\\\n",
    "&=-(\\frac{1}{4})log(\\frac{1}{4})-(\\frac{3}{4})log(\\frac{3}{4})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/4 * np.log2(1/4) - 3/4 * np.log2(3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以定义信息熵计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        ent = 0\n",
    "    else:\n",
    "        ent = -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单测试函数性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，在二分类问题中，$n=2$且$p(x_1)+p(x_2)=1$，我们也可推导二分类的信息熵计算公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) = -p(x)log(p(x))-(1-p(x))log(1-p(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$p(x)$为样本标签为0或1的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.熵的基本性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以证明，熵的计算结果在[0,1]之间，并且熵值越大，系统越混乱、信息越混乱。例如，有如下两个数据集，其中数据集2总共4条样本，0、1类各占50%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>数据集2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|labels|      \n",
    "|:--:|:--:|   \n",
    "|1|0|      \n",
    "|2|1|\n",
    "|3|0|\n",
    "|4|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于该数据集，我们可以计算信息熵为$$H_1(X) = -(\\frac{1}{2})log(\\frac{1}{2})-(\\frac{1}{2})log(\\frac{1}{2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时信息熵达到最高值，也就代表对于上述二分类的数据集，标签随机变量的不确定性已经达到峰值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进一步我们计算下列数据集3的信息熵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>数据集3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|labels|      \n",
    "|:--:|:--:|   \n",
    "|1|1|      \n",
    "|2|1|\n",
    "|3|1|\n",
    "|4|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵计算可得：$$H_1(X) = -(\\frac{4}{4})log(\\frac{4}{4})-(\\frac{0}{4})log(\\frac{0}{4})=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注，信息熵计算中规定$log0=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时信息熵取得最小值，也就代表标签的取值整体呈现非常确定的状态，系统信息规整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 值得一提的是，此时标签本身的信息量也为0，并没有进一步进行预测的必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;结合上述三个数据集，不难看出，当标签取值不均时信息熵较高，标签取值纯度较高时信息熵较低。假设p为未分类数据集中1样本所占比例，则数据集信息熵随着p变化为变化趋势如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(0, 1, 50)\n",
    "ent_l = [entropy(x) for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bfe72b8f08>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'P')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Entropy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(p, ent_l)\n",
    "plt.xlabel('P')\n",
    "plt.ylabel('Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.相对熵（relative entropy）与交叉熵（cross entropy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;相对熵也被称为Kullback-Leibler散度（KL散度）或者信息散度（information divergence）。通常用来衡量两个随机变量分布的差异性。假设对同一个随机变量X，有两个单独的概率分布P(x)和Q(x)，当X是离散变量时，我们可以通过如下相对熵计算公式来衡量二者差异："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{KL}(P||Q)=\\sum ^n_{i=1}P(x_i)log(\\frac{P(x_i)}{Q(x_i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和信息熵类似，相对熵越小，代表Q(x)和P(x)越接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从交叉熵的计算公式不难看出，这其实是一种非对称性度量，也就是$D_{KL}(P||Q)≠D_{KL}(Q||P)$。从本质上来说，相对熵刻画的是用概率分布Q来刻画概率分布P的困难程度，而在机器学习领域，我们一般令Q为模型输出结果，而P为数据集标签真实结果，以此来判断模型输出结果是否足够接近真实情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q为拟合分布P为真实分布，也被称为前向KL散度（forward KL divergence）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，上述相对熵公式等价于："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "D_{KL}(P||Q)&=\\sum ^n_{i=1}P(x_i)log(\\frac{P(x_i)}{Q(x_i)}) \\\\\n",
    "&=\\sum ^n_{i=1}P(x_i)log(P(x_i))-\\sum ^n_{i=1}P(x_i)log(Q(x_i)) \\\\\n",
    "&=-H(P(x))+[-\\sum ^n_{i=1}P(x_i)log(Q(x_i))]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而对于给定数据集，信息熵$H(P(X))$是确定的，因此相对熵的大小完全由$-\\sum ^n_{i=1}P(x_i)log(Q(x_i))$决定。而该式计算结果也被称为交叉熵（cross entropy）计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cross\\_entropy(P,Q) = -\\sum ^n_{i=1}P(x_i)log(Q(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，如果我们希望P、Q二者分布尽可能接近，我们就需要尽可能减少相对熵，但由于相对熵=交叉熵-信息熵，因此我们只能力求减少交叉熵。当然，也正因如此，交叉熵可以作为衡量模型输出分布是否接近真实分布的重要度量方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单总结上述过程要点：\n",
    "- 我们用相对熵$D_{KL}(P||Q)$来表示模型拟合分布Q和数据真实分布P之间的差距，相对熵越小拟合效果越好；\n",
    "- 根据计算公式，$D_{KL}(P||Q)=-H(P(x))+[-\\sum ^n_{i=1}P(x_i)log(Q(x_i))]$，相对熵=交叉熵-信息熵；\n",
    "- 对于给定数据集，信息熵是确定的，因此我们只能通过尽可能减小交叉熵来降低相对熵；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 根据吉布斯不等式，相对熵的取值恒大于等于零，当预测分布和真实分布完全一致时相对熵取值为0，此时交叉熵等于数据信息熵，此外只要二者分布不一致，交叉熵的取值都将大于信息熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单样本交叉熵计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;交叉熵的计算公式看似复杂，但实际运算过程比较简单，对于类似逻辑回归模型输出为连续变量，而真实标签为离散变脸的数据集，可以举例说明计算过程。例如有数据集情况如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|labels|predicts|      \n",
    "|:--:|:--:|:--:|   \n",
    "|1|1|0.8|      \n",
    "|2|0|0.3|\n",
    "|3|0|0.4|\n",
    "|4|1|0.7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将其改写成如下形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|A类|B类|      \n",
    "|:--:|:--:|:--:|   \n",
    "|1|0|1|\n",
    "|predicts|0.2|0.8|\n",
    "|2|1|0|\n",
    "|predicts|0.7|0.3|\n",
    "|3|1|0|\n",
    "|predicts|0.6|0.4|\n",
    "|4|0|1|\n",
    "|predicts|0.3|0.7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中A、B表示每条样本可能所属的类别。围绕该数据集，第一条数据的交叉熵计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cross\\_entropy = -0 * log(0.2)-1*log(0.8)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3219280948873623"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log2(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 再次理解交叉熵计算公式中的叠加是类别的叠加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述数据集标签由0-1转化为A、B，也被称为名义型变量的独热编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多样本交叉熵计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而对于多个数据集，整体交叉熵实际上是每条数据交叉熵的均值。例如上述数据集，整体交叉熵计算结果为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{-1 * log(0.8)\n",
    "-1 * log(0.7)\n",
    "-1 * log(0.6)\n",
    "-1 * log(0.7)}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220100086782713"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-np.log2(0.8)-np.log2(0.7)-np.log2(0.6)-np.log2(0.7)) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;据此，我们可以给出多样本交叉熵计算公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "cross\\_entropy = -\\frac{1}{m}\\sum ^m_j \\sum^n_ip(p_{ij})log(q_{ij})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中m为数据量，n为类别数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对比极大似然估计函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;围绕上述数据集，如果考虑采用极大似然估计来进行计算，我们发现基本计算流程保持一致："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\hat w)= \\sum^N_{i=1}[-y_i \\cdot ln(p_1(\\hat x;\\hat w))-(1-y_i) \\cdot ln(1-p_1(\\hat x;\\hat w))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带入数据可得：\n",
    "$$\n",
    "-ln(0.8)-ln(0.7)-ln(0.6)-ln(0.7)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4473190629576653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.8)-np.log(0.7)-np.log(0.6)-np.log(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管具体数值计算结果有所差异，但基本流程都是类似的——取类别1的概率的对数运算结果进行累加再取负数。因此在实际建模过程中，考虑采用极大似然估计构建损失函数，和采用交叉熵构建损失函数，效果是相同的，二者构建的损失函数都能很好的描绘模型预测结果和真实结果的差异程度。不过在机器学习领域，一般以交叉熵损失函数为主。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二分类交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;据此，我们也可最终推导二分类交叉熵损失函数计算公式，结合极大似然估计的计算公式和交叉熵的基本计算流程，二分类交叉熵损失函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$binaryCE(\\hat w)= -\\frac{1}{n}\\sum^N_{i=1}[y_i \\cdot log(p_1(\\hat x;\\hat w))+(1-y_i) \\cdot log(1-p_1(\\hat x;\\hat w))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以定义一个函数来进行二分类交叉熵损失函数的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE(y, yhat):\n",
    "    \"\"\"\n",
    "    二分类交叉熵损失函数\n",
    "    \"\"\"\n",
    "    return(-(1/len(y))*np.sum(y*np.log2(yhat)+(1-y)*np.log2(1-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单进行验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 0, 1]).reshape(-1, 1)\n",
    "yhat = np.array([0.8, 0.3, 0.4, 0.7]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220100086782713"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCE(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp'至此，我们就完成了完整的逻辑回归损失函数的构建。但正如此前所讨论的一样，对于逻辑回归的损失函数来说，尽管也是凸函数，但无法使用最小二乘法进行求解。在下一节，我们将介绍一种更加通用的求解损失函数的优化算法——梯度下降。并最终完成逻辑回归的参数求解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
